<html>
<!--
    Usage:
        index.html // defaults to WebNN GPU
        index.html?provider=webnn&device=gpu
        index.html?provider=webnn&device=cpu
        index.html?provider=wasm

        provider = wasm | webnn | webgpu | webgl
        device = cpu | gpu | npu // applicable to WebNN
-->

<head>
    <meta charset='utf-8'>
    <title>WebNN Stable Diffusion</title>
    <link rel="apple-touch-icon" sizes="180x180" href="./static/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./static/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./static/favicon/favicon-16x16.png">
    <link rel="manifest" href="./site.webmanifest">
    <link rel="stylesheet" href="./static/main.css">
</head>

<body>
    <div class="grid-2 p1 v1">
        <h1 class="title"><span id="title">WebNN</span> Stable Diffusion
        </h1>
        <p id="error"></p>
    </div>
    <div class="container v9">
        <div class="left">
            <div class="input-group">
                <textarea placeholder="Enter your prompt here" rows="2" type="text"
                    id="positive_prompt">giant castle, mountains, sunrise, volumetric lighting</textarea>
                <div class="tokeninfo">
                    <span id="positive_token_info">63/75 tokens left</span>
                </div>
            </div>
            <div class="input-group">
                <input type="text" id="negative_prompt" placeholder="negative prompt" />
                <div class="tokeninfo negative">
                    <span  id="negative_token_info">75/75 tokens left</span>
                </div>
            </div>
            <div class="input-group grid-4 options">
                <div>
                    <div class="hide">
                        <label for="prompt">Seed</label>
                    </div>
                    <div class="grid-4-41 seed" title="Seed">
                        <input type="text" id="user_seed" value="123465" maxlength="6" minlength="6"></input>
                        <button id="change_seed" value="">
                            <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 -960 960 960" width="24">
                                <path
                                    d="M204-318q-22-38-33-78t-11-82q0-134 93-228t227-94h7l-64-64 56-56 160 160-160 160-56-56 64-64h-7q-100 0-170 70.5T240-478q0 26 6 51t18 49l-60 60ZM481-40 321-200l160-160 56 56-64 64h7q100 0 170-70.5T720-482q0-26-6-51t-18-49l60-60q22 38 33 78t11 82q0 134-93 228t-227 94h-7l64 64-56 56Z" />
                            </svg>
                        </button>
                    </div>
                </div>
                <div></div>
                <div></div>
            </div>
            <div class="button-group key">
                <button id="load_models" class="button">
                    Load Models
                </button>
                <button id="generate_next_image" disabled>
                    <svg xmlns="http://www.w3.org/2000/svg" width="128" height="128" viewBox="0 0 256 256">
                        <path fill="#ffffff"
                            d="m197.58 129.06l-51.61-19l-19-51.65a15.92 15.92 0 0 0-29.88 0L78.07 110l-51.65 19a15.92 15.92 0 0 0 0 29.88L78 178l19 51.62a15.92 15.92 0 0 0 29.88 0l19-51.61l51.65-19a15.92 15.92 0 0 0 0-29.88ZM140.39 163a15.87 15.87 0 0 0-9.43 9.43l-19 51.46L93 172.39a15.87 15.87 0 0 0-9.39-9.39l-51.46-19l51.46-19a15.87 15.87 0 0 0 9.39-9.39l19-51.46l19 51.46a15.87 15.87 0 0 0 9.43 9.43l51.46 19ZM144 40a8 8 0 0 1 8-8h16V16a8 8 0 0 1 16 0v16h16a8 8 0 0 1 0 16h-16v16a8 8 0 0 1-16 0V48h-16a8 8 0 0 1-8-8m104 48a8 8 0 0 1-8 8h-8v8a8 8 0 0 1-16 0v-8h-8a8 8 0 0 1 0-16h8v-8a8 8 0 0 1 16 0v8h8a8 8 0 0 1 8 8" />
                    </svg>
                    Generate Image
                </button>
            </div>
            <div class="grid-2">
                <div class="progress">
                    <div class="progress-bar">
                        <div id="progress-bar-inner" class="progress-bar-inner"></div>
                    </div>
                    <div class="progress-bar-label" id="progress-bar-label">0%</div>
                </div>
                <div class="progress">
                    <div class="progress-bar">
                        <div id="progress-bar-inner-inference" class="progress-bar-inner"></div>
                    </div>
                    <div class="progress-bar-label" id="progress-bar-label-inference">0%</div>
                </div>
            </div>

            <div class="log-output" id='status'>
            </div>
            <div id="data" class="hide">
                <table>
                    <tr>
                        <th class="category"></th>
                        <th class="load">Load</th>
                        <th class="load">Model Fetch</th>
                        <th class="load">Session Create</th>
                        <th class="run">Execution (incl. Session Run)</th>
                    </tr>
                    <tr id="textencoder">
                        <td title="235MB">Text Encoder</td>
                        <td id="textencoderload"></td>
                        <td id="textencoderfetch" title="235MB"></td>
                        <td id="textencodercreate"></td>
                        <td id="textencoderrun"></td>
                    </tr>
                    <tr id="unet">
                        <td title="1.60GB">UNet</td>
                        <td id="unetload"></td>
                        <td id="unetfetch" title="1.60GB"></td>
                        <td id="unetcreate"></td>
                        <td id="unetrun"></td>
                    </tr>
                    <tr id="vaedecoder">
                        <td title="94.5MB">VAE Decoder</td>
                        <td id="vaedecoderload"></td>
                        <td id="vaedecoderfetch" title="94.5MB"></td>
                        <td id="vaedecodercreate"></td>
                        <td id="vaedecoderrun"></td>
                    </tr>
                    <tr id="total">
                        <td>Total</td>
                        <td id="totalload"></td>
                        <td id="totalfetch">-</td>
                        <td id="totalcreate">-</td>
                        <td id="totalrun"></td>
                    </tr>
                </table>
            </div>
            <div id="webnnstatus"><span id="circle"></span> <span id="info">WebNN</span></div>
            <div id="ortversion"></div>
        </div>
        <div class="right">
            <div>
                <canvas class='canvas' id='canvas' width='512' height='512'>
                    Canvas is not supported. You'll need to try a newer browser version or another browser.
                </canvas>
            </div>
        </div>
    </div>
</body>

<script type='module'>
    import * as Utils from './utils.js';

    // Configuration...
    const pixelWidth = 512;
    const pixelHeight = 512;
    const latentWidth = (pixelWidth / 8);
    const latentHeight = (pixelHeight / 8);
    const latentChannelCount = 4;
    const unetBatch = 2;
    const unetChannelCount = 4;
    const textEmbeddingSequenceLength = 77;
    const textEmbeddingSequenceWidth = 768;
    const unetIterationCount = 25; // Hard-coded number of samples, since the denoising weight ramp is constant.
    let seed = BigInt(123465);
    let performanceData = {
        loadtime: {
            textencoder: 0,
            unet: 0,
            vaedecoder: 0,
            total: 0
        },
        modelfetch: {
            textencoder: 0,
            unet: 0,
            vaedecoder: 0,
        },
        sessioncreate: {
            textencoder: 0,
            unet: 0,
            vaedecoder: 0,
        },
        sessionrun: {
            textencoder: 0,
            unet: [],
            unettotal: 0,
            vaedecoder: 0,
            total: 0
        }
    };

    function to(promise, errorExt) {
        return promise
            .then(function (data) { return [null, data]; })
            .catch(function (err) {
            if (errorExt) {
                Object.assign(err, errorExt);
            }
            return [err, undefined];
        });
    }

    const setupORT = async () => {
        const ortversion = document.querySelector('#ortversion');
        Utils.removeElement('onnxruntime-web');
        let ortVersion = await Utils.getOrtDevVersion();
        let ortLink = '';
        if (ortVersion && ortVersion.length > 4) {
            await Utils.loadScript('onnxruntime-web', `https://cdn.jsdelivr.net/npm/onnxruntime-web@${ortVersion}/dist/ort.all.min.js`);
            ortLink = `https://www.npmjs.com/package/onnxruntime-web/v/${ortVersion}`
            ortversion.innerHTML = `ONNX Runtime Web: <a href="${ortLink}">${ortVersion}</a>`;
        } else {
            await Utils.loadScript('onnxruntime-web', './static/ort/ort.all.min.js');
            ortversion.innerHTML = `ONNX Runtime Web: Internal version`;
        }
    }

    let progress = 0;
    let fetchProgress = 0;
    let textEncoderFetchProgress = 0;
    let unetFetchProgress = 0;
    let vaeDecoderFetchProgress = 0;

    // Get model via Origin Private File System
    async function getModelOPFS(name, url, updateModel) {
        const root = await navigator.storage.getDirectory();
        let fileHandle;

        async function updateFile() {
            const response = await fetch(url);
            const buffer = await readResponse(name, response);
            fileHandle = await root.getFileHandle(name, { create: true });
            const writable = await fileHandle.createWritable();
            await writable.write(buffer);
            await writable.close();
            return buffer;
        }

        if (updateModel) {
            return await updateFile();
        }

        try {
            fileHandle = await root.getFileHandle(name);
            const blob = await fileHandle.getFile();
            let buffer = await blob.arrayBuffer();
            if(buffer) {
                if (name == 'text-encoder') {
                    textEncoderFetchProgress = 10;
                } else if (name == 'stable-diffusion-unet') {
                    unetFetchProgress = 80;
                } else if (name == 'stable-diffusion-vae-decoder') {
                    vaeDecoderFetchProgress = 5;
                }

                progress = textEncoderFetchProgress + unetFetchProgress + vaeDecoderFetchProgress;
                progressBarInner.style.width = progress + "%";

                if (name == 'text-encoder') {
                    progressBarLabel.textContent = "Loading Text Encoder model · 235MB · " + progress.toFixed(2) + "%";
                } else if (name == 'stable-diffusion-unet') {
                    progressBarLabel.textContent = "Loading UNet model · 1.60GB · " + progress.toFixed(2) + "%";
                } else if (name == 'stable-diffusion-vae-decoder') {
                    progressBarLabel.textContent = "Loading VAE Decoder model · 94.5MB · " + progress.toFixed(2) + "%";
                }

                return buffer;
            }

        } catch (e) {
            return await updateFile();
        }
    }

    async function readResponse(name, response) {
        const contentLength = response.headers.get('Content-Length');
        let total = parseInt(contentLength ?? '0');
        let buffer = new Uint8Array(total);
        let loaded = 0;

        const reader = response.body.getReader();
        async function read() {
            const { done, value } = await reader.read();
            if (done) return;

            let newLoaded = loaded + value.length;
            fetchProgress = (newLoaded / contentLength) * 100;
            
            if (name == 'text-encoder') {
                textEncoderFetchProgress = 0.1 * fetchProgress;
            } else if (name == 'stable-diffusion-unet') {
                unetFetchProgress = 0.8 * fetchProgress;
            } else if (name == 'stable-diffusion-vae-decoder') {
                vaeDecoderFetchProgress = 0.05 * fetchProgress;
            }

            progress = textEncoderFetchProgress + unetFetchProgress + vaeDecoderFetchProgress;
            progressBarInner.style.width = progress + "%";

            if (name == 'text-encoder') {
                progressBarLabel.textContent = "Loading Text Encoder model · 235MB · " + progress.toFixed(2) + "%";
            } else if (name == 'stable-diffusion-unet') {
                progressBarLabel.textContent = "Loading UNet model · 1.60GB · " + progress.toFixed(2) + "%";
            } else if (name == 'stable-diffusion-vae-decoder') {
                progressBarLabel.textContent = "Loading VAE Decoder model · 94.5MB · " + progress.toFixed(2) + "%";
            }

            if (newLoaded > total) {
                total = newLoaded;
                let newBuffer = new Uint8Array(total);
                newBuffer.set(buffer);
                buffer = newBuffer;
            }
            buffer.set(value, loaded);
            loaded = newLoaded;
            return read();
        }

        await read();
        return buffer;
    }

    Utils.log('[Load] Loading ONNX Runtime');
    const progressBarInner = document.getElementById("progress-bar-inner");
    const progressBarLabel = document.getElementById("progress-bar-label");
    const progressBarInnerInference = document.querySelector("#progress-bar-inner-inference");
    const progressBarLabelInference = document.querySelector("#progress-bar-label-inference");
    const startButton = document.getElementById('generate_next_image');
    const loadButton = document.getElementById('load_models');
    const logOutput = document.getElementById("status");
    const positiveInput = document.getElementById('positive_prompt');
    const negativeInput = document.getElementById('negative_prompt');
    const positiveTokenInfo = document.getElementById('positive_token_info');
    const negativeTokenInfo = document.getElementById('negative_token_info');
    const error = document.querySelector('#error');
    const userSeed = document.querySelector('#user_seed');
    const changeSeed = document.querySelector('#change_seed');
    const title = document.querySelector('#title');
    const data = document.querySelector('#data');
    const textEncoderLoad = document.querySelector('#textencoderload');
    const textEncoderFetch = document.querySelector('#textencoderfetch');
    const textEncoderCreate = document.querySelector('#textencodercreate');
    const textEncoderRun = document.querySelector('#textencoderrun');
    const unetLoad = document.querySelector('#unetload');
    const unetFetch = document.querySelector('#unetfetch');
    const unetCreate = document.querySelector('#unetcreate');
    const unetRun = document.querySelector('#unetrun');
    const vaeDecoderLoad = document.querySelector('#vaedecoderload');
    const vaeDecoderFetch = document.querySelector('#vaedecoderfetch');
    const vaeDecoderCreate = document.querySelector('#vaedecodercreate');
    const vaeDecoderRun = document.querySelector('#vaedecoderrun');
    const totalLoad = document.querySelector('#totalload');
    const totalRun = document.querySelector('#totalrun');
    let inferenceProgress = 0;

    loadButton.onclick = async () => {
        progress = 0;
        fetchProgress = 0;
        textEncoderFetchProgress = 0;
        unetFetchProgress = 0;
        vaeDecoderFetchProgress = 0;

        data.removeAttribute('class');
        data.setAttribute('class', 'hide');

        performanceData.loadtime.textencoder = 0;
        performanceData.loadtime.unet = [];
        performanceData.loadtime.vaedecoder = 0;
        performanceData.loadtime.total = 0

        performanceData.modelfetch.textencoder = 0;
        performanceData.modelfetch.unet = 0;
        performanceData.modelfetch.vaedecoder = 0;

        performanceData.sessioncreate.textencoder = 0;
        performanceData.sessioncreate.unet = 0;
        performanceData.sessioncreate.vaedecoder = 0;

        loadButton.disabled = true;
        startButton.disabled = true;
        await loadStableDiffusion(executionProvider);
        startButton.disabled = false;

        if (performanceData.loadtime.total) {
            textEncoderLoad.innerHTML = performanceData.loadtime.textencoder
            textEncoderFetch.innerHTML = performanceData.modelfetch.textencoder;
            textEncoderCreate.innerHTML = performanceData.sessioncreate.textencoder;
            textEncoderRun.innerHTML = '-';

            unetLoad.innerHTML = performanceData.loadtime.unet;
            unetFetch.innerHTML = performanceData.modelfetch.unet;
            unetCreate.innerHTML = performanceData.sessioncreate.unet;
            unetRun.innerHTML =  '-';

            vaeDecoderLoad.innerHTML = performanceData.loadtime.vaedecoder;
            vaeDecoderFetch.innerHTML = performanceData.modelfetch.vaedecoder;
            vaeDecoderCreate.innerHTML = performanceData.sessioncreate.vaedecoder;
            vaeDecoderRun.innerHTML = '-';

            totalLoad.innerHTML = performanceData.loadtime.total;
            totalRun.innerHTML = '-';
        }

        data.setAttribute('class', 'show');
    }

    startButton.onclick = async () => {
        performanceData.sessionrun.textencoder = 0;
        performanceData.sessionrun.unet = [];
        performanceData.sessionrun.unettotal = 0;
        performanceData.sessionrun.vaedecoder = 0;
        performanceData.sessionrun.total = 0

        startButton.disabled = true;
        await generateNextImage();
        inferenceProgress = 0;
    }

    positiveInput.addEventListener('input', async (e) => {
        const inputValue = e.target.value;
        const ids = await Utils.getTokenizers(inputValue);
        // Max token length is 75.
        const left_tokens_length = 75 - ids.length;
        positiveTokenInfo.innerHTML = `${(left_tokens_length <= 0) ? 0 : left_tokens_length}/75`;
    });

    negativeInput.addEventListener('input', async (e) => {
        const inputValue = e.target.value;
        const ids = await Utils.getTokenizers(inputValue);
        // Max token length is 75.
        const left_tokens_length = 75 - ids.length;
        negativeTokenInfo.innerHTML = `${(left_tokens_length <= 0) ? 0 : left_tokens_length}/75`;
    });

    async function getTextTokens() {
        const positiveText = positiveInput.value;
        const negativeText = negativeInput.value;

        // A string like 'a cute magical flying ghost dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure'
        // becomes a 1D tensor of {49406, 320, 2242, 7823, 4610, 7108, 1929, 267, 5267, 794, 267, 3878, 3140, 267, 1400, 3027, ...}
        // padded with blanks (id 49407) up to the maximum sequence length of the text encoder (typically 77).
        // So the text encoder can't really handle more than 75 words (+1 start, +1 stop token),
        // not without some extra tricks anyway like calling it multiple times and combining the embeddings.
        let positive_token_ids = [49406]; // Inits with start token
        let negative_token_ids = [49406];
        const positive_text_ids = await Utils.getTokenizers(positiveText);
        positive_token_ids = positive_token_ids.concat(positive_text_ids);
        if (positive_text_ids.length > (textEmbeddingSequenceLength - 2)) { // Max inputs ids should be 75
            positive_token_ids = positive_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            positive_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - positive_token_ids.length).fill(49407);
            positive_token_ids = positive_token_ids.concat(fillerArray);
        }

        let negative_text_ids = await Utils.getTokenizers(negativeText);
        negative_token_ids = negative_token_ids.concat(negative_text_ids);
        if (negative_text_ids.length > (textEmbeddingSequenceLength - 2)) {
            negative_token_ids = negative_token_ids.slice(0, (textEmbeddingSequenceLength - 1));
            negative_token_ids.push(49407);
        } else {
            const fillerArray = new Array(textEmbeddingSequenceLength - negative_token_ids.length).fill(49407);
            negative_token_ids = negative_token_ids.concat(fillerArray);
        }

        const token_ids = positive_token_ids.concat(negative_token_ids);
        return token_ids;
    };

    Utils.log('[Load] ONNX Runtime loaded');

    function convertPlanarFloat16RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (Utils.decodeFloat16(input[redInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (Utils.decodeFloat16(input[greenInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (Utils.decodeFloat16(input[blueInputOffset + i]) + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarUint8RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            let inputValue = input[redInputOffset + i];
            rgba[j + 0] = inputValue;
            rgba[j + 1] = inputValue;
            rgba[j + 2] = inputValue;
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    function convertPlanarFloat32RgbToUint8Rgba(input /*Uint16Array*/, width, height) {
        let totalPixelCount = width * height;
        let totalOutputBytes = totalPixelCount * 4;

        let redInputOffset = 0;
        let greenInputOffset = redInputOffset + totalPixelCount;
        let blueInputOffset = greenInputOffset + totalPixelCount;

        const rgba = new Uint8ClampedArray(totalOutputBytes);
        for (let i = 0, j = 0; i < totalPixelCount; i++, j += 4) {
            rgba[j + 0] = (input[redInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 1] = (input[greenInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 2] = (input[blueInputOffset + i] + 1.0) * (255.0 / 2.0);
            rgba[j + 3] = 255;
        }
        return rgba;
    }

    async function loadModel(modelName/*:String*/, executionProvider/*:String*/) {
        let modelPath;
        let modelSession;
        let freeDimensionOverrides;
        let modelSize;

        if (modelName == 'text-encoder') {
            modelSize = '235MB'; 
        } else if (modelName == 'stable-diffusion-unet') {
            modelSize = '1.60GB'; 
        } else if (modelName == 'stable-diffusion-vae-decoder') {
            modelSize = '94.5MB'; 
        }

        Utils.log(`[Load] Loading model ${modelName} · ${modelSize}`);
        if (modelName == 'text-encoder') {
            //  Inputs:
            //    int32 input_ids[batch,sequence]
            //    batch: 2
            //    sequence: 77
            //  Outputs:
            //    float16 last_hidden_state[Addlast_hidden_state_dim_0,Addlast_hidden_state_dim_1,768]
            //    float16 pooler_output[Addlast_hidden_state_dim_0,768] We don't care about this ignorable output.
            //    Addlast_hidden_state_dim_0: 2
            //    Addlast_hidden_state_dim_1: 77
            // modelPath = 'models/Stable-Diffusion-v1.5-text-encoder-float16.onnx';
            modelPath = Utils.modelPath() + 'text-encoder.onnx'
            freeDimensionOverrides = {
                'batch': unetBatch,
                'sequence': textEmbeddingSequenceLength,
            }
        }
        else if (modelName == 'stable-diffusion-unet') {
            //  Typical shapes (some models may vary, like inpainting have 9 channels or single batch having 1 batch)...
            //
            //  Inputs:
            //    float16 sample[2, 4, 64, 64]
            //    int64 timestep[2]
            //    float16 encoder_hidden_states[2, 77, 768]
            //  Outputs:
            //    float16 out_sample[2, 4, 64, 64]
            modelPath = Utils.modelPath() + 'sd-unet-v1.5-model-b2c4h64w64s77-float16-compute-and-inputs.onnx';

            freeDimensionOverrides =
            {
                'batch': unetBatch,
                'channels': unetChannelCount,
                'height': latentHeight,
                'width': latentWidth,
                'sequence': textEmbeddingSequenceLength,
                'unet_sample_batch': unetBatch,
                'unet_sample_channels': unetChannelCount,
                'unet_sample_height': latentHeight,
                'unet_sample_width': latentWidth,
                'unet_time_batch': unetBatch,
                'unet_hidden_batch': unetBatch,
                'unet_hidden_sequence': textEmbeddingSequenceLength,
            };
        }
        else if (modelName == 'stable-diffusion-vae-decoder') {
            //  Inputs:
            //    float16 latent_sample[1, 4, 64, 64]
            //  Outputs:
            //    float16 sample[1, 3, 512, 512]
            modelPath = Utils.modelPath() + 'Stable-Diffusion-v1.5-vae-decoder-float16-fp32-instancenorm.onnx';
            freeDimensionOverrides = { 'batch': 1, 'channels': latentChannelCount, 'height': latentHeight, 'width': latentWidth, };
        }
        else {
            throw new Error(`Model ${modelName} is unknown`);
        }

        const options =
        {
            executionProviders: [{ name: executionProvider, deviceType: Utils.getQueryVariable('device', 'gpu'), powerPreference: 'default' }],
        };

        if (freeDimensionOverrides != undefined) {
            options.freeDimensionOverrides = freeDimensionOverrides;
        }

        options.logSeverityLevel = 3;

        Utils.log('[Load] Model path = ' + modelPath);
        let modelBuffer;

        let fetchStartTime = performance.now();
        modelBuffer = await getModelOPFS(modelName, modelPath, false);
        let fetchTime = (performance.now() - fetchStartTime).toFixed(2);

        if (modelName == 'text-encoder') {
            performanceData.modelfetch.textencoder = fetchTime;
            progressBarLabel.textContent = `Loaded Text Encoder · ${(fetchTime / 1000).toFixed(2)}s · 10%`;
            Utils.log(`[Load] Text Encoder loaded · ${(fetchTime / 1000).toFixed(2)}s`);

            progressBarLabel.textContent = 'Creating session for Text Encoder · 10%';
            Utils.log('[Session Create] Beginning Text encode');
        } else if (modelName == 'stable-diffusion-unet') {
            performanceData.modelfetch.unet = fetchTime;
            progressBarLabel.textContent = `Loaded UNet · ${(fetchTime / 1000).toFixed(2)}s · 90%`;
            Utils.log(`[Load] UNet loaded · ${(fetchTime / 1000).toFixed(2)}s`);

            progressBarLabel.textContent = 'Creating session for UNet · 90%'
            Utils.log('[Session Create] Beginning UNet');
        } else if (modelName == 'stable-diffusion-vae-decoder') {
            performanceData.modelfetch.vaedecoder = fetchTime;
            progressBarLabel.textContent = `Loaded VAE Decoder · ${(fetchTime / 1000).toFixed(2)}s · 95%`;
            Utils.log(`[Load] VAE Decoder loaded · ${(fetchTime / 1000).toFixed(2)}s`);

            progressBarLabel.textContent = 'Creating session for VAE Decoder · 95%'
            Utils.log('[Session Create] Beginning VAE decode');
        }

        let createStartTime = performance.now();
        modelSession = await ort.InferenceSession.create(modelBuffer, options);
        
        if (modelName == 'text-encoder') {
            let textencoderCreateTime = (performance.now() - createStartTime).toFixed(2);
            performanceData.sessioncreate.textencoder = textencoderCreateTime;
            progressBarLabel.textContent = `Text Encoder session created · ${textencoderCreateTime}ms · 10%`
            Utils.log(`[Session Create] Text Encoder Completed · ${textencoderCreateTime}ms`);
        } else if (modelName == 'stable-diffusion-unet') {
            let unetCreateTime = (performance.now() - createStartTime).toFixed(2);
            performanceData.sessioncreate.unet = unetCreateTime;
            progressBarLabel.textContent = `UNet session created · ${unetCreateTime}ms · 90%`
            Utils.log(`[Session Create] UNet Completed · ${unetCreateTime}ms`);
        } else if (modelName == 'stable-diffusion-vae-decoder') {
            let vaedecoderCreateTime = (performance.now() - createStartTime).toFixed(2);
            performanceData.sessioncreate.vaedecoder = vaedecoderCreateTime;
            progressBarLabel.textContent = `VAE Decoder session created · ${vaedecoderCreateTime}ms · 95%`
            Utils.log(`[Session Create] VAE Decoder Completed · ${vaedecoderCreateTime}ms`);
        }
        return modelSession;
    }

    function displayEmptyCanvasPlaceholder() {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');
        context.fillStyle = 'rgba(255, 255, 255, 0.5)';
        context.strokeStyle = 'rgba(255, 255, 255, 0.0)';
        context.lineWidth = 0;
        //context.fillRect(0, 0, pixelWidth, pixelHeight);
        context.textAlign = 'center';
        context.textBaseline = 'middle';
        context.font = '300px sans-serif';
        context.fillText('🖼️', canvas.width / 2, canvas.height / 2);
        context.strokeRect(0, 0, pixelWidth, pixelHeight);
    }

    function displayPlanarRGB(planarPixelData/*: Float32Array or Uint16Array as float16 or Uint8Array*/) {
        const canvas = document.getElementById('canvas');
        const context = canvas.getContext('2d');

        // TODO: See if ORT's toImageData() is flexible enough to handle this instead.
        // It doesn't appear work correctly, just returning all white (shrug, maybe I'm passing the wrong values).
        // https://onnxruntime.ai/docs/api/js/interfaces/Tensor-1.html#toImageData
        // https://github.com/microsoft/onnxruntime/blob/5228332/js/common/lib/tensor-conversion.ts#L33
        // https://github.com/microsoft/onnxruntime/blob/main/js/common/lib/tensor-factory.ts#L147
        //
        // let imageData = planarPixelTensor.toImageData({format: 'RGB', tensorLayout: 'NCHW', norm:{bias: 1, mean: 128}});

        let conversionFunction = planarPixelData instanceof Float32Array
            ? convertPlanarFloat32RgbToUint8Rgba
            : planarPixelData instanceof Uint16Array
                ? convertPlanarFloat16RgbToUint8Rgba
                : convertPlanarUint8RgbToUint8Rgba;

        let rgbaPixels = conversionFunction(planarPixelData, pixelWidth, pixelHeight);

        let imageData = new ImageData(rgbaPixels, pixelWidth, pixelHeight);
        context.putImageData(imageData, 0, 0);
    }

    let textEncoderSession;
    let vaeDecoderModelSession;
    let unetModelSession;

    // Hard-coded values for 25 iterations (the standard).
    const defaultSigmas/*[25 + 1]*/ = [14.614647, 11.435942, 9.076809, 7.3019943, 5.9489183, 4.903778, 4.0860896, 3.4381795, 2.9183085, 2.495972, 2.1485956, 1.8593576, 1.6155834, 1.407623, 1.2280698, 1.0711612, 0.9323583, 0.80802417, 0.695151, 0.5911423, 0.49355352, 0.3997028, 0.30577788, 0.20348993, 0.02916753, 0.0];
    const defaultTimeSteps/*[25]*/ = [999.0, 957.375, 915.75, 874.125, 832.5, 790.875, 749.25, 707.625, 666.0, 624.375, 582.75, 541.125, 499.5, 457.875, 416.25, 374.625, 333.0, 291.375, 249.75, 208.125, 166.5, 124.875, 83.25, 41.625, 0.0];

    async function initializeOnnxRuntime() {
        // Global singletons -_-. Initialize ORT's global singleton.
        ort.env.wasm.numThreads = 1; // 4
        ort.env.wasm.simd = true;
        ort.env.wasm.proxy = false;
    }

    async function loadStableDiffusion(executionProvider) {
        try {
            // Release sessions if load models again.
            if (textEncoderSession) {
                await unetModelSession.release();
                await textEncoderSession.release();
                await vaeDecoderModelSession.release();
            }

            error.removeAttribute("class");
            error.innerHTML = '';
            
            const loadStartTime = performance.now();
            textEncoderSession = await loadModel('text-encoder', executionProvider);
            performanceData.loadtime.textencoder = (performance.now() - loadStartTime).toFixed(2);
            
            const unetLoadStartTime = performance.now();
            unetModelSession = await loadModel('stable-diffusion-unet', executionProvider);
            performanceData.loadtime.unet = (performance.now() - unetLoadStartTime).toFixed(2);

            const vaeDecoderLoadStartTime = performance.now();
            vaeDecoderModelSession = await loadModel('stable-diffusion-vae-decoder', executionProvider);
            performanceData.loadtime.vaedecoder = (performance.now() - vaeDecoderLoadStartTime).toFixed(2);

            progress += 5;
            progressBarInner.style.width = progress + "%";
            progressBarLabel.textContent = "Models loaded and sessions created · " + progress.toFixed(2) + "%";
            const loadTime = performance.now() - loadStartTime;
            Utils.log(`[Total] Total load time (models load and sessions creation): ${(loadTime / 1000).toFixed(2)}s`);
            performanceData.loadtime.total = loadTime.toFixed(2);
            startButton.removeAttribute('disabled');
        }
        catch (e) {
            console.log('Exception: ', e);
            error.setAttribute("class", "error");
            error.innerHTML = e.message;
            Utils.appendStatus('Exception: ' + e);
        }
    }

    function practRandSimpleFastCounter32(a, b, c, d)
    // https://pracrand.sourceforge.net/
    // Using this as a substitute for std::minstd_rand instead.
    // (std::linear_congruential_engine<std::uint_fast32_t, 48271, 0, 2147483647>).
    {
        return function () {
            a >>>= 0; b >>>= 0; c >>>= 0; d >>>= 0;
            var t = (a + b) | 0;
            a = b ^ b >>> 9;
            b = c + (c << 3) | 0;
            c = (c << 21 | c >>> 11);
            d = d + 1 | 0;
            t = t + d | 0;
            c = c + t | 0;
            return (t >>> 0) / 4294967296;
        };
    }

    function generateNoise(/*out*/ latentSpace/*: Uint16Array*/, seed/*: BigInt*/) {
        // Don't know nearly equivalent to .

        let randomGenerator = practRandSimpleFastCounter32(
            Number(seed >> 0n) & 0xFFFFFFFF,
            Number(seed >> 32n) & 0xFFFFFFFF,
            Number(seed >> 64n) & 0xFFFFFFFF,
            Number(seed >> 96n) & 0xFFFFFFFF
        );

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            const u1 = randomGenerator();
            const u2 = randomGenerator();
            const radius = Math.sqrt(-2.0 * Math.log(u1));
            const theta = 2.0 * Math.PI * u2;
            const standardNormalRand = radius * Math.cos(theta);
            const newValue = standardNormalRand;
            latentSpace[i] = Utils.encodeFloat16(newValue);
        }
    }

    function prescaleLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, initialSigma/*: float*/) {
        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * initialSigma);
        }
    }

    function scaleLatentSpaceForPrediction(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: int*/) {
        console.assert(iterationIndex < defaultSigmas.length);

        // sample = sample / ((sigma**2 + 1) ** 0.5)
        let sigma = defaultSigmas[iterationIndex];
        let inverseScale = 1 / Math.sqrt(sigma * sigma + 1);

        const elementCount = latentSpace.length;
        for (let i = 0; i < elementCount; ++i) {
            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) * inverseScale);
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two batches, with the positive prediction in batch 0, negative in batch 1.
    function denoiseLatentSpace(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, predictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === predictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.
        const singleBatchElementCount = elementCount / 2; // Given [2, 4, 64, 64], we want only the first batch.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        for (let i = 0; i < singleBatchElementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(predictedNoise[i]) * positiveWeight + Utils.decodeFloat16(predictedNoise[i + singleBatchElementCount]) * negativeWeight;

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
        }
    }

    // Adjusts the latent space in-place by the predicted noise, weighted for the current iteration.
    // This version takes two separate predicted noise arrays.
    function denoiseLatentSpaceSplitPredictions(/*inout*/ latentSpace/*: Uint16Array*/, iterationIndex/*: Number*/, positivePredictedNoise/*: Uint16Array*/, negativePredictedNoise/*: Uint16Array*/) {
        console.assert(latentSpace.length === positivePredictedNoise.length);
        console.assert(latentSpace.length === negativePredictedNoise.length);

        const elementCount = latentSpace.length;          // Given [2, 4, 64, 64], count of all elements.

        // Prompt strength scale.
        const defaultPromptStrengthScale = 7.5;
        const positiveWeight = defaultPromptStrengthScale;
        const negativeWeight = 1 - positiveWeight;

        // Add predicted noise (scaled by current iteration weight) to latents.
        const sigma = defaultSigmas[iterationIndex];
        const sigmaNext = defaultSigmas[iterationIndex + 1];
        const dt = sigmaNext - sigma;

        for (let i = 0; i < elementCount; ++i) {
            // Fold 2 batches into one, weighted by positive and negative weights.
            const weightedPredictedNoise = Utils.decodeFloat16(positivePredictedNoise[i]) * positiveWeight + Utils.decodeFloat16(negativePredictedNoise[i]) * negativeWeight;

            // The full formula:
            //
            //  // 1. Compute predicted original sample from sigma-scaled predicted noise.
            //  float sample = latentSpace[i];
            //  float predictedOriginalSample = sample - sigma * predictedNoiseData[i];
            // 
            //  // 2. Convert to an ODE derivative
            //  float derivative = (sample - predictedOriginalSample) / sigma;
            //  float previousSample = sample + derivative * dt;
            //  latentSpace[i] = previousSample;
            //
            // Simplifies to:
            //
            //  updatedSample = sample + ((sample - (sample - sigma * predictedNoiseData[i])) / sigma  * dt);
            //  updatedSample = sample + ((sample - sample + sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + ((sigma * predictedNoiseData[i]) / sigma  * dt);
            //  updatedSample = sample + (predictedNoiseData[i] * dt);

            latentSpace[i] = Utils.encodeFloat16(Utils.decodeFloat16(latentSpace[i]) + weightedPredictedNoise * dt);
        }
    }

    function applyVaeScalingFactor(latentSpace/*: Uint16Array as float16*/) {
        const /*float*/ defaultVaeScalingFactor = 0.18215; // Magic constants for default VAE :D (used in Huggingface pipeline).
        const /*float*/ inverseScalingFactor = 1.0 / defaultVaeScalingFactor;
        latentSpace.forEach((e, i, a) => a[i] = Utils.encodeFloat16(Utils.decodeFloat16(e) * inverseScalingFactor));
    }

    async function executeStableDiffusion()/*: ort.Tensor*/
    // Implicit inputs:
    // - unetModelSession
    // - unetInputs
    // - vaeDecoderInputs
    {
        Utils.log('[Session Run] Beginning Text encode');
        let token_ids = await getTextTokens();
        const startTextEncoder = performance.now();
        const textEncoderInputs = {
            'input_ids': Utils.generateTensorFromValues('int32', [unetBatch, textEmbeddingSequenceLength], token_ids),
        };
        const textEncoderOutputs = await textEncoderSession.run(textEncoderInputs);

        let textEncoderExecutionTime = (performance.now() - startTextEncoder).toFixed(2);
        performanceData.sessionrun.textencoder = textEncoderExecutionTime;
        Utils.log(`[Session Run] Text Encode execution time: ${textEncoderExecutionTime}ms`);

        inferenceProgress += 1;
        progressBarInnerInference.style.width = inferenceProgress + "%";
        progressBarLabelInference.textContent = "Text encoded · " + inferenceProgress.toFixed(2) + "%";

        Utils.log('[Session Run] Beginning UNet loop execution for 25 iterations');

        let latentSpace = new Uint16Array(latentWidth * latentHeight * unetChannelCount);
        generateNoise(/*inout*/ latentSpace, seed);
        // Duplicate the input data, once for each batch (only supports unetBatch == 2).
        latentSpace = new Uint16Array([...latentSpace, ...latentSpace]);

        const latentsTensor = Utils.generateTensorFromBytes(
            'float16',
            [unetBatch, unetChannelCount, latentHeight, latentWidth],
            latentSpace);

        const halfLatentElementCount = latentsTensor.size / 2; // Given [2, 4, 64, 64], we want only the first batch.
        let latents = await latentsTensor.getData();
        let halfLatents = latents.subarray(0, halfLatentElementCount); // First batch only.
        prescaleLatentSpace(/*inout*/ halfLatents, defaultSigmas[0]);

        const unetInputs = {
            'encoder_hidden_states': Utils.generateTensorFromBytes('float16',
                [unetBatch, textEmbeddingSequenceLength, textEmbeddingSequenceWidth],
                textEncoderOutputs['last_hidden_state'].data),
        };

        const startUnet = performance.now();
        // Repeat unet detection and denosing until convergence (typically 25 iterations).
        for (var i = 0; i < unetIterationCount; ++i) {
            // Update time step.
            let startUnetIteration = performance.now();
            const timeStepValue = BigInt(Math.round(defaultTimeSteps[i])); // Round, because this ridiculous language throws an exception otherwise.
            unetInputs['timestep'] = Utils.generateTensorFillValue('int64', [unetBatch], timeStepValue);

            // Prescale the latent values.
            // Copy first batch to second batch, duplicating latents for positive and negative prompts.
            let nextLatents = latents.slice(0);
            let halfNextLatents = nextLatents.subarray(0, halfLatentElementCount);
            scaleLatentSpaceForPrediction(/*inout*/ halfNextLatents, i);
            nextLatents.copyWithin(halfLatentElementCount, 0, halfLatentElementCount); // Copy lower half to upper half.

            unetInputs['sample'] = Utils.generateTensorFromBytes('float16', [unetBatch, unetChannelCount, latentHeight, latentWidth], nextLatents);
            const unetOutputs = await unetModelSession.run(unetInputs);

            let predictedNoise = new Uint16Array(unetOutputs['out_sample'].cpuData.buffer);
            denoiseLatentSpace(/*inout*/ latents, i, predictedNoise);

            let time = (performance.now() - startUnetIteration).toFixed(2);
            performanceData.sessionrun.unet.push(time);
            // Utils.log(`UNet loop ${i + 1} execution time: ${time}ms`);

            inferenceProgress += 3.8;
            progressBarInnerInference.style.width = inferenceProgress + "%";
            progressBarLabelInference.textContent = `UNet iteration ${i + 1} completed · ${inferenceProgress.toFixed(2)}%`;
        }

        let unetExecutionTime = (performance.now() - startUnet).toFixed(2);
        performanceData.sessionrun.unettotal = unetExecutionTime;
        Utils.log(`[Session Run] UNet loop execution time: ${unetExecutionTime}ms`);

        Utils.log('[Session Run] Beginning VAE decode');
        // Decode from latent space.
        applyVaeScalingFactor(/*inout*/ halfLatents);
        let dimensions = latentsTensor.dims.slice(0);
        dimensions[0] = 1; // Set batch size to 1, ignore the 2nd batch for the negative prediction.

        const startVaeDecoder = performance.now();
        const vaeDecoderInputs = {
            'latent_sample': Utils.generateTensorFromBytes('float16', dimensions, halfLatents.slice(0)),
        };
        const decodedOutputs = await vaeDecoderModelSession.run(vaeDecoderInputs);
        let vaeDecoderExecutionTime = (performance.now() - startVaeDecoder).toFixed(2);
        Utils.log(`[Session Run] VAE decode execution time: ${vaeDecoderExecutionTime}ms`);
        performanceData.sessionrun.vaedecoder = vaeDecoderExecutionTime;

        inferenceProgress += 4;
        progressBarInnerInference.style.width = inferenceProgress + "%";
        progressBarLabelInference.textContent = "VAE decoded · " + inferenceProgress.toFixed(2) + "%";

        return decodedOutputs['sample'];
    }

    async function executeStableDiffusionAndDisplayOutput() {
        try {

            error.removeAttribute("class");
            error.innerHTML = '';
            displayEmptyCanvasPlaceholder();

            const executionStartTime = performance.now();
            let rgbPlanarPixels = await executeStableDiffusion();
            const executionTime = performance.now() - executionStartTime;
            performanceData.sessionrun.total = executionTime.toFixed(2);
            Utils.log(`[Total] Total execution time: ${(executionTime / 1000).toFixed(2)}s`);
            console.log(performanceData);
            displayPlanarRGB(await rgbPlanarPixels.getData());
        }
        catch (e) {
            error.setAttribute("class", "error");
            error.innerHTML = e.message;
            console.log('Exception: ', e);
            Utils.appendStatus('Exception: ' + e);
        }
    }

    async function generateNextImage() {
        await executeStableDiffusionAndDisplayOutput();
        // seed++;
        console.log(seed);
        startButton.disabled = false;

        if (performanceData.sessionrun.total) {
            textEncoderLoad.innerHTML = performanceData.loadtime.textencoder
            textEncoderFetch.innerHTML = performanceData.modelfetch.textencoder;
            textEncoderCreate.innerHTML = performanceData.sessioncreate.textencoder;
            textEncoderRun.innerHTML = performanceData.sessionrun.textencoder;

            unetLoad.innerHTML = performanceData.loadtime.unet;
            unetFetch.innerHTML = performanceData.modelfetch.unet;
            unetCreate.innerHTML = performanceData.sessioncreate.unet;
            unetRun.innerHTML = performanceData.sessionrun.unet.toString().replaceAll(',', ' ') + '<br/>25 Iterations: ' + performanceData.sessionrun.unettotal;

            vaeDecoderLoad.innerHTML = performanceData.loadtime.vaedecoder;
            vaeDecoderFetch.innerHTML = performanceData.modelfetch.vaedecoder;
            vaeDecoderCreate.innerHTML = performanceData.sessioncreate.vaedecoder;
            vaeDecoderRun.innerHTML = performanceData.sessionrun.vaedecoder;

            totalLoad.innerHTML = performanceData.loadtime.total;
            totalRun.innerHTML = performanceData.sessionrun.total;
        }

        data.setAttribute('class', 'show');
    }

    const executionProvider = Utils.getQueryVariable('provider', 'webnn');
    Utils.log('[Load] Execution Provider: ' + executionProvider);

    const checkWebNN = async () => {
        let status = document.querySelector('#webnnstatus');
        let info = document.querySelector('#info');
        let webnnStatus = await Utils.webNnStatus();

        if (webnnStatus.webnn) {
            status.setAttribute('class', 'green');
            info.innerHTML = 'WebNN supported · 6GB available memory required';
        } else {
            if (webnnStatus.error) {
                status.setAttribute('class', 'red');
                info.innerHTML = 'WebNN not supported: ' + webnnStatus.error;
            } else {
                status.setAttribute('class', 'red');
                info.innerHTML = 'WebNN not supported';
            }
        }

        if (Utils.getQueryValue('provider') && Utils.getQueryValue('provider').toLowerCase().indexOf('webgpu') > -1) {
            status.innerHTML = '';
        }
    };

    const ui = async () => {
        await setupORT();
        if (Utils.getQueryValue('provider') && Utils.getQueryValue('provider').toLowerCase().indexOf('webgpu') > -1) {
            title.innerHTML = 'WebGPU';
        }
        await checkWebNN();
        initializeOnnxRuntime();
        displayEmptyCanvasPlaceholder();
    };

    document.addEventListener('DOMContentLoaded', ui, false);

    const updateSeed = () => {
        userSeed.value = Utils.randomNumber();
        seed = BigInt(userSeed.value);
    }

    changeSeed.addEventListener('click', updateSeed, false);
</script>

</html>